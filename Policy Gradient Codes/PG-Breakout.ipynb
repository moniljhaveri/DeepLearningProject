{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pickle\n",
    "\n",
    "# Initialize OpenAI for Breakout\n",
    "env = gym.make(\"Breakout-v0\")\n",
    "\n",
    "# Hyper parameters for policy gradient model\n",
    "global num_actions, discount, batch_size\n",
    "num_actions = 3 # env.action_space.n # 6\n",
    "discount = 0.99\n",
    "save_every = 10\n",
    "train_every = 1\n",
    "reward_factor = 1\n",
    "max_episode = 100000 # When to terminate traininig\n",
    "initial_exp = -1 # Initial exploration probability\n",
    "final_exp = 0.01 # Final exploration probability\n",
    "\n",
    "# Hyper parameters for Network\n",
    "learning_rate=0.0005\n",
    "decay_ = 0.95\n",
    "epsilon_ = 1e-8\n",
    "FC_param = {'FC_1':600}\n",
    "\n",
    "# Initialize parameter for policy gradient model \n",
    "observation = env.reset()\n",
    "image_old  = None\n",
    "images, fake_labels, rewards_std, action_hist, reward_hist, reward_runn = [], [], [], [], [], []\n",
    "reward_episode = 0\n",
    "\n",
    "# Build network\n",
    "def Network(image_, FC_param):\n",
    "    \n",
    "    FC_tmp = tf.contrib.layers.flatten(image_, scope='Flatten')\n",
    "    \n",
    "    for key,value in FC_param.items():\n",
    "        FC_tmp = tf.layers.dense(inputs=FC_tmp, \n",
    "                                 units=value,\n",
    "                                 kernel_initializer= tf.truncated_normal_initializer(mean=0,\n",
    "                                                                                     stddev=1./np.sqrt(5000), \n",
    "                                                                                     dtype=tf.float32),\n",
    "                                 activation=tf.nn.relu,\n",
    "                                 use_bias=False,\n",
    "                                 name=key)\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=FC_tmp, \n",
    "                             units=num_actions,\n",
    "                             kernel_initializer= tf.truncated_normal_initializer(mean=0,\n",
    "                                                                                 stddev=1./np.sqrt(500), \n",
    "                                                                                 dtype=tf.float32),\n",
    "                             use_bias=False,\n",
    "                             name='Logits')\n",
    "    \n",
    "    action_probs = tf.nn.softmax(logits, name='SoftMax')\n",
    "    \n",
    "    return action_probs\n",
    "\n",
    "# Get discounted reward and normalize it\n",
    "def discount_norm(rew):\n",
    "    rew_func = lambda a, v: a*discount + v # Reward function\n",
    "    \n",
    "    rew_reverse = tf.scan(rew_func, tf.reverse(rew,[True, False]))\n",
    "    discounted_rew = tf.reverse(rew_reverse,[True, False])\n",
    "    \n",
    "    mean, variance= tf.nn.moments(discounted_rew, [0])\n",
    "    discounted_rew -= mean\n",
    "    discounted_rew /= tf.sqrt(variance + 1e-6)\n",
    "    \n",
    "    return discounted_rew\n",
    "\n",
    "# Process image by cropping and binarizing\n",
    "def process_obs(obs):\n",
    "    obs = obs[32:196,8:152]\n",
    "    obs = obs[::2,::2,0]\n",
    "    obs[obs != 0] = 1 \n",
    "    return obs.astype(np.float)\n",
    "\n",
    "# Calculate running mean for plotting\n",
    "def get_running_mean(reward_hist):\n",
    "    \n",
    "    running_mean=[]\n",
    "    mean=0\n",
    "    for i in range(len(reward_hist)):\n",
    "        mean = 0.99*mean + 0.01*reward_hist[i]\n",
    "        running_mean.append(mean)\n",
    "        \n",
    "    return running_mean\n",
    "\n",
    "# Build model\n",
    "with tf.Graph().as_default() as g:\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        image_ = tf.placeholder(dtype=tf.float32, shape=[None, 82, 72,1],name=\"image\")\n",
    "        fake_label_ = tf.placeholder(dtype=tf.float32, shape=[None, num_actions],name=\"fake_label\")\n",
    "        reward_ = tf.placeholder(dtype=tf.float32, shape=[None,1], name=\"reward\")\n",
    "        \n",
    "        # Get policy gradient\n",
    "        discounted_epr = discount_norm(reward_) \n",
    "\n",
    "        tf_aprob = Network(image_,FC_param)\n",
    "        loss = tf.nn.l2_loss(fake_label_-tf_aprob) # Define loss\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=decay_, epsilon=epsilon_)\n",
    "        \n",
    "        # Assign optimizer with artificial gradient\n",
    "        grads = optimizer.compute_gradients(loss, var_list=tf.trainable_variables(), grad_loss=discounted_epr)\n",
    "        \n",
    "        train_op = optimizer.apply_gradients(grads)\n",
    "\n",
    "# Main program\n",
    "with g.as_default(), tf.Session() as sess:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    \n",
    "    # Load from last session or start over\n",
    "    try:\n",
    "        episode_number = pickle.load(open('PG-Breakout-ckpt-1/last-episode.p','rb'))\n",
    "        saver.restore(sess, 'PG-Breakout-ckpt-1\\\\Breakout.ckpt-'+str(episode_number))\n",
    "        reward_hist = pickle.load(open('PG-Breakout-ckpt-1/all_reward.p','rb'))\n",
    "        print('Continue from last saved episode')\n",
    "    except:\n",
    "        print('Training from beginning')\n",
    "        episode_number = 0\n",
    "        \n",
    "    # Starting playing\n",
    "    while episode_number<=max_episode: \n",
    "        \n",
    "        # Process observation\n",
    "        image_proc = process_obs(observation)\n",
    "        image_diff = image_proc - image_old if image_old is not None else np.zeros((82, 72))\n",
    "        image_old = image_proc\n",
    "\n",
    "        # Uniformly pick an action\n",
    "        feed_step = {image_: np.reshape(image_diff, (1,82, 72,1))}\n",
    "        aprob = sess.run(tf_aprob,feed_step) ; aprob = aprob[0,:]\n",
    "        \n",
    "        # Epsilon greedy exploration\n",
    "        ratio = np.max((max_episode - episode_number)/max_episode, 0)\n",
    "        exploration_prob = (initial_exp - final_exp) * ratio + final_exp\n",
    "        if random.random() < exploration_prob:\n",
    "            action = random.randint(0, num_actions-1)\n",
    "        else:\n",
    "            action = action = np.random.choice(num_actions, p=aprob)\n",
    "        \n",
    "        # Generate fake label for back prop\n",
    "        label = np.zeros_like(aprob) ; label[action] = 1\n",
    "\n",
    "        # Input action to OpenAI and get feedback\n",
    "        observation, reward, done, info = env.step(action+1)\n",
    "        reward_episode += reward\n",
    "\n",
    "        # Record for training\n",
    "        images.append(image_diff); fake_labels.append(label); rewards_std.append(reward); action_hist.append(action+1)\n",
    "\n",
    "        # Training\n",
    "        if done:\n",
    "            \n",
    "            if episode_number % train_every == 0:\n",
    "                \n",
    "                feed_episode = {image_: np.array(images).reshape(-1, 82, 72, 1), \n",
    "                        fake_label_: np.array(fake_labels).reshape(-1,num_actions), \n",
    "                        reward_: np.array([i * reward_factor for i in rewards_std]).reshape(-1,1)}\n",
    "                sess.run(train_op,feed_episode)\n",
    "                    \n",
    "                # Clear memory\n",
    "                episode_duration = len(rewards_std)\n",
    "                images, fake_labels, rewards_std, action_hist = [], [], [], []\n",
    "            \n",
    "            # Bookkeeping\n",
    "            reward_hist.append(reward_episode)\n",
    "            reward_runn.append(reward_episode)\n",
    "            episode_number += 1\n",
    "            observation = env.reset()\n",
    "            print('\\tep {}: reward: {} duration: {}'.format(episode_number, reward_episode,episode_duration))\n",
    "            reward_episode = 0\n",
    "            \n",
    "            # Save model\n",
    "            if episode_number % save_every == 0:\n",
    "                saver.save(sess, 'PG-Breakout-ckpt-1/Breakout.ckpt', global_step=episode_number)\n",
    "                print('Model saved, mean reward is',np.mean(reward_runn))\n",
    "                pickle.dump(episode_number,open('PG-Breakout-ckpt-1/last-episode.p','wb'))\n",
    "                pickle.dump(reward_hist,open('PG-Breakout-ckpt-1/all_reward.p','wb'))\n",
    "                plt.figure(figsize=(18,3))\n",
    "                plt.plot([7]*(len(reward_hist)-55000),'r--')\n",
    "                plt.plot(np.array(get_running_mean(reward_hist[55000:episode_number])))\n",
    "                plt.show()\n",
    "                reward_runn = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "colors = cm.rainbow(np.linspace(0, 1, 20))\n",
    "interval = 1000\n",
    "\n",
    "def get_mean_var(reward_hist):\n",
    "    ts = pd.Series(reward_hist)\n",
    "    return pd.rolling_mean(ts, interval), pd.rolling_var(ts, interval)\n",
    "\n",
    "running_mean, running_var = get_mean_var(reward_hist)\n",
    "\n",
    "plt.title('Breakout result',fontsize=20)\n",
    "plt.ylim(0,60)\n",
    "plt.plot(np.array(reward_hist),label='All rewards',c=colors[3,:])\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Rewards')\n",
    "plt.plot(np.array(running_mean),label='Running mean',c='y')\n",
    "plt.plot([7]*len(running_mean),'r--')\n",
    "plt.legend(loc=1,fontsize=10)\n",
    "plt.savefig('PG-Breakout-ckpt-1/ANN1.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
